import time
import random
import numpy as np
from deap import base
from deap import creator
from deap import tools
import json

from ml_pentest.file_manipulations.pe_format.section_injection import SectionInjection
from ml_pentest.attacks.blackbox.genetic_attack.genetic_engine.genetic_algorithm_base import GeneticEngine
from ml_pentest.attacks.blackbox.genetic_attack.GAMMA.gamma_attack import GammaAttack
from ml_pentest.attacks.blackbox.genetic_attack.GAMMA.gamma_result import GammaResult


class GammaSectionInjection(GammaAttack, GeneticEngine):
    """
    GammaSectionInjection is a genetic algorithm-based attack on machine learning models used for malware classification.
    This class inherits functionalities from both the GammaAttack and GeneticEngine classes.

    Attributes:
        _section_population_length (int): Number of benign sections available for injection.
        _stagnation (int): Number of iterations without improvement before stopping the attack.
        _input_sample (np.ndarray): The input sample to be attacked.
        _section_population (list): List of benign sections to be used for injection.
        _manipulator (SectionInjection): Object responsible for applying the manipulations.
        (Other attributes inherited from GammaAttack and GeneticEngine classes.)

    Methods:
        __init__(): Initializes the GammaSectionInjection object with given parameters.
        initialization(): Initializes the genetic algorithm.
        termination_condition(): Checks if the termination condition has been met.
        iteration(): Performs one iteration of the genetic algorithm.
        _evaluate(): Evaluates the fitness of each individual in the population.
        exiting_operations(): Performs exiting operations and returns the result.
        compute_fitness_function(): Computes the fitness function for a given sample and manipulation vector.
        _compute_loss(): Computes the loss function based on the chosen loss type.
        compute_reg_term(): Computes the regularization term of the fitness function.
        compute_confidence(): Computes the confidence score of the classification function.
        run(): Runs the genetic algorithm and returns the result.
        get_attack_characteristic(): Returns a string with all attack characteristics in JSON format.
    """

    def __init__(self, section_population, model_wrapper , population_size, lambda_value, iterations, input_sample=None, query_budget=None, seed=None, debug=False, hard_label=False, threshold=0.5, loss='l1', stagnation=5):
        """
        Initializes the GammaSectionInjection object with the given parameters.

        Args:
            section_population (list): A list of Section objects representing the benign sections to use as candidates in the genetic algorithm.
            model_wrapper (ModelWrapper): The classifier to be attacked with classification function.
            population_size (int): The size of the population of individuals in the genetic algorithm.
            lambda_value (float): The lambda value used in the fitness function.
            iterations (int): number of maximum iterations of the genetic algorithm. If query_budget is used, the maximum number 
                              of iterations will be limited by the query budget.
            input_sample (np.ndarray): The input sample to be attacked. Default to None, if you want only to initialize the attack object.
            query_budget (int): The maximum number of queries allowed to the classifier. Defaults to None, which means 
                                that there is no limit to the number of queries.
            seed (int): The seed to be used in the random number generator.
            debug (bool): Whether to print debug information.
            hard_label (bool): Whether to use hard labels in the fitness function.
            threshold (float): The threshold used to detect a sample as a malware. Tipically, if the
                               confidence of the classifier is > 0.5, this is a malware, but you can 
                               change the value if this is not the case. This value is used only in hard label mode.
            loss (str): The loss function used in the fitness function to combine confidence and penalty term. With 
                        'l1' the loss is the absolute value of the difference in bytes between original samples and adversarial one. 
                        The other values are:
                        - 'cw', in which the loss is computed as max(confidence - self.threshold + 0.1, 0) + penalty.
                        - 'log', in which the loss is computed as -log(1 - confidence) + penalty.
            stagnation (int): the number of iteration without improvement before stopping the attack. Defaults to 5.
        Returns:
            None
        """
        super().__init__(model_wrapper , population_size,
                         lambda_value, iterations, query_budget, seed, debug, hard_label, threshold, loss)
        # Number of section to inject in the input sample
        self._section_population_length = len(section_population)
        # number of iterations without improvement before stopping
        self._stagnation = stagnation
        self._input_sample = input_sample
        self._section_population = section_population
        self._manipulator = SectionInjection(self._section_population, debug=False)

    def initialization(self):
        """
        Initializes the genetic algorithm by creating the necessary objects and registering the required functions
        with the DEAP toolbox. It also creates a population of individuals and evaluates their fitness.

        Returns:
            None
        """
        if self._input_sample is None:
            raise ValueError(
                "Input sample is not provided! Use the method set_sample to set it before running attack.")
        self._start_time = time.time()
        if self._debug:
            print("==== Initialization phase ====")
        ################################### INITIALIZE DHE DEAP TOOLBOX ###################################
        # creates a Fitness object that minimizes fitness. The weights argument is used to define the sign of the fitness.
        creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
        # creates an Individual class that is a list of floats with the fitness attribute set to the FitnessMin class
        #  The "Individual" class represents a potential solution to the problem being solved (a candidate solution)
        creator.create("Individual", list, fitness=creator.FitnessMin)
        # Create a toolbox for evolution that contains the evolutionary operators
        toolbox = base.Toolbox()
        #  Register a function that generates a random float between 0 and 1 by using the random.random method
        toolbox.register(alias='attr_float', function=random.random)
        # Creating an individual with a list of float attributes (with population_size length) between 0 and 1 and registering
        # it in the toolbox for use in the genetic algorithm. A candidate is represented by a list
        # s = [0.1 0.32 ...] where s[i] is the percentage of the section to inject into the input sample.
        toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float,
                         n=self._section_population_length)
        # Create a population of individuals and registering it in the toolbox for use in the genetic algorithm.
        toolbox.register("population", tools.initRepeat,
                         list, toolbox.individual)
        # Register the evaluate method to the fitness function to use into the algorithm.
        #toolbox.register("evaluate", self.compute_fitness_function)
        # Function performs one-point crossover on two individuals using the cxOnePoint method
        toolbox.register("crossover", tools.cxOnePoint)
        # Mutate function mutates an individual with a probability of 0.3 using the random_mutation method.
        toolbox.register("mutate", random_mutation, indpb=0.3)
        # Selects the best individuals from a population using the selTournament method
        toolbox.register("select", tools.selTournament,
                         tournsize=self._population_size)
        # create the random immigrates object
        toolbox.register("random_immigrates", tools.initRepeat, list, toolbox.individual)
        # saves the toolbox object into the class attribute
        self._toolbox = toolbox

        # The _slice_indexes list is used to slice the population into smaller subpopulations.
        # The single element in the list represents the end index of the first subpopulation,
        # which is equal to the total population size.
        self._slice_indexes = [self._population_size]
        ###############################################################################
        # Creates a population of individuals and evaluates their fitness on the input sample.
        self._population = self._toolbox.population(n=self._population_size)

        # initialize this tuple that contain the best fit and the best candidate actually discovered
        self._best_candidate = (np.infty, None)
        self._evaluate(self._population)
        # CXPB  is the probability with which two individuals are crossed
        self._CXPB = 0.3
        # MUTPB is the probability for mutating an individual
        self._MUTPB = 0.2
        # this is the index of the current iteration
        self._iteration = 0

        # The _last_n_best_fits list is used to keep track of the best fitness values of the last n iterations,
        # where n is the stagnation parameter passed to the constructor of the GammaSectionInjection class.
        # This list is used to check if the algorithm has stagnated, i.e., if the best fitness value has not
        # improved for n iterations.
        self._last_n_best_fits = []

    def termination_condition(self):
        """
        Check if the termination condition has been met.

        Returns:
            bool: True if the termination condition has been met, False otherwise.
        """
        if len(self._last_n_best_fits) == self._stagnation and (
                all((np.array(self._last_n_best_fits) - self._best_fitness) < 1e-6) or all(
                np.array(self._last_n_best_fits) == np.infty)):
            if self._debug:
                print('Stagnating result!')
            return True
        if not self._query_budget:
            if self._debug and self._iteration >= self._iterations:
                print("\nTerminating condition met! g:{}, iterations:{}".format(
                    self._iteration, self._iterations))
            return self._iteration >= self._iterations
        if self._debug and ((self._iteration >= self._iterations) or ((self._iteration+1)*self._population_size >= self._query_budget)):
            print("\nTerminating condition met! g:{}, iterations:{}, query budget {}".format(
                self._iteration, self._iterations, self._query_budget))
        return (self._iteration >= self._iterations) or ((self._iteration+1)*self._population_size >= self._query_budget)

    def iteration(self):
        """
        Perform one iteration of the genetic algorithm.

        Returns:
            None
        """
        if self._debug:
            print("\n==== Iteration {} ====".format(self._iteration + 1))
        ############## SELECTION ################
        # Select the next generation individuals
        selected_individuals = self._toolbox.select(
            self._population, self._population_size)
        # Clone the selected individuals
        selected_individuals = list(
            map(self._toolbox.clone, selected_individuals))
        ############## CROSSOVER ################
        i = 0
        # Apply crossover and mutation on the selected individuals
        for child1, child2 in zip(selected_individuals[::2], selected_individuals[1::2]):
            if random.random() < self._CXPB:
                self._toolbox.crossover(child1, child2)
                i += 2
                del child1.fitness.values
                del child2.fitness.values
        ############## MUTATION ################
        for mutant in selected_individuals:
            if random.random() < self._MUTPB:
                i += 1
                self._toolbox.mutate(mutant)
                del mutant.fitness.values
        ############# RANDOM IMMIGRATES ################
        if i < self._population_size:
            random_immigrates = self._toolbox.random_immigrates(n=self._population_size - i)
            if self._debug:
                print("Number of random immigrates: ", len(random_immigrates))

        # Evaluate the individuals with an invalid fitness
        # (i.e. the ones that have been mutated or crossed in the previous step or the random immigrates)
        to_evaluate = [ind for ind in selected_individuals if not ind.fitness.valid]
        if i < self._population_size:
            to_evaluate.extend(random_immigrates)
        # This line of code appends the length of the to_evaluate list to the _slice_indexes list.
        # This is required because _slice_indexes is used to keep track of the indexes of the individuals
        # in the population that belong to each section.
        # By appending the length of to_evaluate, we are essentially marking the end of the current section and
        # the beginning of the next section in the population. This is important for the selection process,
        # as it ensures that individuals from each section are selected with equal probability.
        self._slice_indexes.append(len(to_evaluate))

        self._evaluate(to_evaluate)

        self._population.extend(to_evaluate)
        # Compute values to determine stagnation condition
        fits = [candidate.fitness.values[0] for candidate in self._population]
        self._best_fitness = min(fits)
        self._last_n_best_fits.insert(0, self._best_fitness)
        self._last_n_best_fits = self._last_n_best_fits[:self._stagnation]

        if self._debug:
            print(f'>{self._iteration} - Global min: {self._best_fitness}')
        self._iteration += 1

    def _evaluate(self, to_evaluate):
        """
        Evaluates the fitness of each individual in the population by computing the fitness function for each individual.

        Parameters
        ----------
        to_evaluate : list
            A list of individuals to evaluate.

        Returns
        -------
        None
        """
        if self._debug:
            print("Evaluating {} individuals".format(len(to_evaluate)))
        if not self._model_wrapper._use_batch:
            #fitness = [[self.compute_fitness_function(self._input_sample, np.array(candidate))] for candidate in to_evaluate]
            
            fitness = []
            for candidate in to_evaluate:
                fit_value, adv_sample = self.compute_fitness_function(self._input_sample, np.array(candidate))
                # Save fit value
                fitness.append([fit_value])
                # Save best candidate if it is better than the current one
                if fit_value < self._best_candidate[0]:
                    self._best_candidate = (fit_value, adv_sample)
        else:
            adv_samples = []
            reg_terms = []
            for manipulation_vector in to_evaluate:
                self._manipulator.set_pe_file(self._input_sample)
                adv_sample = self._manipulator.apply_manipulation(manipulation_vector)
                adv_samples.append(bytes(adv_sample))
                reg_terms.append(self.compute_reg_term(manipulation_vector=manipulation_vector, section_population=self._section_population))
            confidences = self._model_wrapper.classify_batch(adv_samples)
            fitness = []
            for i in range(len(to_evaluate)):
                fitness.append([self._compute_loss(confidences[i], reg_terms[i])])
        
        if self._debug:
            print("Fitness values: {}".format(fitness))
        for candidate, fit in zip(to_evaluate, fitness):
            candidate.fitness.values = fit
                
            

    def exiting_operations(self):
        """
        Performs the exiting operations of the genetic algorithm, including selecting the best candidate, applying the
        manipulation to the input sample, and returning a GammaResult object with the manipulated sample and other
        relevant information.

        Returns
        -------
        GammaResult
            A GammaResult object containing the manipulated sample and other relevant information.
        """
        del creator.FitnessMin
        del creator.Individual
        best_candidate = self._best_candidate[1]
        if self._debug:
            print("Best fit: ", self._best_candidate[0])
            if best_candidate is not None:
                print("Confidence: ", self.compute_confidence(bytes(best_candidate)))
        return GammaResult(best_candidate, time.time() - self._start_time, self._iteration, (self._iteration+1) * self._population_size, len(self._last_n_best_fits) == self._stagnation and (
             all(np.array(self._last_n_best_fits) == np.infty) or all((np.array(self._last_n_best_fits) - self._best_fitness) < 1e-6) ))

    def compute_fitness_function(self, sample, manipulation_vector):
        """
        Computes the fitness function for a given sample and manipulation vector. The fitness function is defined as the
        sum of the classification score and the regularization term.

        Parameters
        ----------
        sample : numpy.ndarray
            The array that represent the file to evaluate as byte arrays.
        manipulation_vector : list
            A list of floats representing the manipulation vector.

        Returns
        -------
        float
            The value of the fitness function.
        """
        self._manipulator.set_pe_file(sample)
        adv_sample = self._manipulator.apply_manipulation(manipulation_vector)
        return self._compute_loss(self.compute_confidence(bytes(adv_sample)), self.compute_reg_term(manipulation_vector, self._section_population)), adv_sample

    def _compute_loss(self, confidence, penalty):
        """
        Computes the loss function for a given confidence score and penalty value. The loss function is defined based on
        the chosen loss type, which can be 'l1', 'cw', or 'log'.

        Parameters
        ----------
        confidence : float
            The confidence score of the classification function.
        penalty : float
            The penalty value to be added to the loss function.

        Returns
        -------
        float
            The value of the loss function.
        """
        if self._loss == 'l1':
            return confidence + penalty
        if self._loss == 'cw':
            return max(confidence - self._threshold + 0.1, 0) + penalty
        if self._loss == 'log':
            return -np.log(1 - confidence) + penalty
        raise ValueError('NO LOSS')

    def compute_reg_term(self, manipulation_vector, section_population):
        """
        Computes the regularization term of the fitness function, which is lambda times the sum of the number of bytes
        injected in each section of the binary. The number of injected bytes is computed by multiplying the manipulation
        vector with the corresponding section population.

        Parameters
        ----------
        manipulation_vector : list
            A list of floats representing the manipulation vector.
        section_population : list
            A list of integers representing the population of each section of the binary.

        Returns
        -------
        float
            The value of the regularization term.
        """

        injected_bytes = 0
        for i in range(len(manipulation_vector)):
            injected_bytes += manipulation_vector[i] * \
                len(section_population[i])
        return self._lambda * injected_bytes

    def compute_confidence(self, sample):
        """Compute f(sample), where f is the classification function"""
        confidence = self._model_wrapper.classify_sample(sample)
        #print("Confidence: ", confidence)
        if self._hard_label:
            return np.infty if confidence > self._threshold else 0
        return confidence

    def run(self):
        """
        Runs the genetic algorithm by performing initialization, iterations, and exiting operations until the termination
        condition is met. Returns the result of the exiting operations and sets the elapsed time attribute.

        :return: The result of the exiting operations and the elapsed time.
        """
        self.initialization()
        while not self.termination_condition():
            self.iteration()
        result = self.exiting_operations()
        return result
    
    def get_attack_characteristic(self):
        """Return a string with all attacks characteristics in JSON format.
        """
        attack_characteristics = {
            "attack": self.__class__.__name__,
            "population_size": self._population_size,
            "lambda": self._lambda,
            "max_iterations": self._iterations,
            "query_budget": self._query_budget,
            "seed": self._seed,
            "hard_label": self._hard_label,
            "threshold": self._threshold,
            "loss": self._loss,
            "stagnation": self._stagnation,
            "section_population_length": self._section_population_length
            }
        return json.dumps(attack_characteristics)


def random_mutation(individual, indpb):
    """
    Apply the mutation operator, that perturb randomly each entry of the individual, with a given probability.
    The mutation is applied in-place.

    Parameters
    ----------
    individual :
            the individual to mutate
    indpb : float
            the probability of altering a single entry
    Returns
    -------
    tuple
            the mutated individual, the mutatio is in-place
    """
    size = len(individual)
    for i in range(size):
        if random.random() < indpb:
            individual[i] = random.random()
    return individual
